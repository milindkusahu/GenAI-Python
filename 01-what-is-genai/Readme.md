# ðŸŒ Introduction to AI World

- **AI** predicts based on the **input** provided by the user.
- **OpenAI â†’ GPT (Generative Pre-trained Transformer)**: A language model trained on vast text data.

### Types:

- **ChatGPT**: GPT + Tools/Agents â†’ can access **real-time data** âœ…
- **GPT API**: Pure model â†’ **no real-time access** âŒ

---

## âš™ï¸ Behind the Scenes â€“ How it Works

### ðŸ§  Phase 1: Input and Encoding

1. **Input Query** â†’ `"Man is to King as Woman is to ?"`
2. **Tokenization**: Converts words into numerical tokens for model processing  
   _Example_: `"Queen"` â†’ `[5001]`
3. **Mathematics**: These tokens go through neural network layers.
4. **De-tokenization**: Numbers back to human-readable text.

ðŸ”— **Try it**: [Tiktokenizer Tool](https://tiktokenizer.vercel.app/) â€“ See how tokens map to words.

---

## ðŸ” Vector Embeddings

- **Embeddings** represent words as **vectors** in multi-dimensional space.
- Similar meanings = Vectors close to each other.

### ðŸ“Œ Example from Image:

```text
Man - Woman â‰ˆ King - Queen
```

- This is a popular analogy visualized using **word vectors**.
- AI uses these **relationships** to understand context.

ðŸ§  Think:  
If `"Man"` â†’ `"King"`  
Then `"Woman"` â†’ `"Queen"`

---

## ðŸ§­ Positional Encoding

> Helps the model understand **word order** since transformers don't process tokens sequentially.

- Each word gets a **position-based encoding** added to its embedding.

### ðŸ§ª Example:

```text
"The cat sat on the mat" â‰  "The mat sat on the cat"
```

Even with same words, **position changes meaning**, so token positions affect understanding.

---

## ðŸ”„ Self-Attention Mechanism

> Allows the model to **weigh the importance** of other tokens when encoding a particular token.

ðŸ§  Tokens "talk" to each other.

### ðŸ§ª Example:

```text
"The river bank" â†’ "bank" relates to water  
"The ICICI bank" â†’ "bank" relates to finance
```

Self-attention uses **context** to resolve meaning.

---

## ðŸ§ª Phase: Inferencing & Training

### ðŸ”§ Training
1. Input: `<start> My name is Milind`
2. Output: model predicts next word â†’ compares with actual â†’ calculates **loss**
3. **Backpropagation** corrects weights.

```text
<start> My name is Milind â†’ END  
     â†“  
Loss â†’ Backprop â†’ Adjust weights
```

### ðŸ”„ Inference
1. Input token â†’ Pass through layers
2. **Linear â†’ Softmax â†’ Probability distribution**
3. **Temperature** controls randomness in output:
   - `Low temp` (e.g. 0.2) â†’ more predictable
   - `High temp` (e.g. 1.0) â†’ more creative

---
